# Modelos de Difusi√≥n: Gu√≠a Educativa Completa

Este directorio contiene materiales educativos sobre **Modelos de Difusi√≥n**, dise√±ados espec√≠ficamente para aprendices de Machine Learning.

## üìö Contenido

### Notebooks Educativos

1. **[01_diffusion_fundamentals.ipynb](01_diffusion_fundamentals.ipynb)** - Fundamentos desde Cero
   - Implementaci√≥n completa de un modelo de difusi√≥n
   - Forward y reverse process
   - Entrenamiento en MNIST
   - Generaci√≥n de im√°genes desde ruido

2. **[02_text_to_image_diffusion.ipynb](02_text_to_image_diffusion.ipynb)** - Modelos Texto-a-Imagen
   - Arquitectura de DALL-E 2, Stable Diffusion, Imagen
   - CLIP text encoder
   - Classifier-free guidance
   - Uso pr√°ctico de Stable Diffusion

3. **[diffusion_models_presentation.ipynb](diffusion_models_presentation.ipynb)** - Presentaci√≥n General
   - Historia de los modelos de difusi√≥n
   - Conceptos b√°sicos
   - Aplicaciones

---

## üéØ Entendiendo el Forward Process

### La F√≥rmula Fundamental

La f√≥rmula que controla c√≥mo a√±adimos ruido gradualmente es:

$$q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t} x_{t-1}, \beta_t I)$$

### ¬øQu√© significa cada parte?

#### **$q(x_t | x_{t-1})$** - Probabilidad Condicional
- Se lee: "La probabilidad de obtener $x_t$ **dado que** ya tenemos $x_{t-1}$"
- Describe c√≥mo pasamos de un paso al siguiente

#### **$\mathcal{N}(x_t; \mu, \sigma^2)$** - Distribuci√≥n Normal
- $x_t$ sigue una distribuci√≥n gaussiana
- **Media**: $\mu = \sqrt{1-\beta_t} x_{t-1}$
- **Varianza**: $\sigma^2 = \beta_t I$

#### **$\sqrt{1-\beta_t} x_{t-1}$** - Factor de Encogimiento
- Toma la imagen anterior y la "encoge" ligeramente
- Si $\beta_t = 0.01$, entonces $\sqrt{1-\beta_t} = \sqrt{0.99} = 0.995$
- La imagen se multiplica por 0.995 (reducci√≥n del 0.5%)

#### **$\beta_t I$** - Varianza del Ruido
- $\beta_t$: Cu√°nto ruido a√±adimos en este paso
- $I$: Matriz identidad (ruido independiente en cada pixel)
- Desviaci√≥n est√°ndar del ruido: $\sqrt{\beta_t}$

### Forma Pr√°ctica de la F√≥rmula

En c√≥digo, esto se traduce a:

$$x_t = \sqrt{1-\beta_t} \cdot x_{t-1} + \sqrt{\beta_t} \cdot \epsilon$$

donde $\epsilon \sim \mathcal{N}(0, I)$ es ruido est√°ndar (media 0, varianza 1)

---

## üîë Concepto Clave: Control del Ruido

### ¬øPor qu√© esta f√≥rmula es "controlada"?

La f√≥rmula garantiza que el ruido agregado sea **controlado y predecible** para evitar que la imagen se corrompa ca√≥ticamente. Esto se logra gracias a usar una **distribuci√≥n normal (gaussiana)**.

#### 1. **Corrupci√≥n Gradual, No Ca√≥tica**

```
‚ùå Sin control: x_t = x_{t-1} + ruido_aleatorio_cualquiera
   Problema: Ruido impredecible, valores pueden explotar

‚úÖ Con control: x_t = ‚àö(1-Œ≤‚Çú) ¬∑ x_{t-1} + ‚àöŒ≤‚Çú ¬∑ Œµ
   Soluci√≥n: Œ≤‚Çú controla exactamente cu√°nto ruido a√±ades
```

#### 2. **Varianza Constante = No Explosi√≥n**

Si simplemente sum√°ramos ruido en cada paso:
- El ruido se **acumular√≠a**
- Los valores podr√≠an ir a ¬±‚àû
- La imagen "explotar√≠a" matem√°ticamente

**Con nuestra f√≥rmula**, la varianza se mantiene constante (‚âà1):

$$\text{Var}(x_t) = (1-\beta_t) \text{Var}(x_{t-1}) + \beta_t \text{Var}(\epsilon) = 1$$

Despu√©s de 1000 pasos:
- Los valores siguen en un rango razonable
- La imagen se convierte en **ruido puro** controlado, no en caos infinito

---

## üîî ¬øPor qu√© la Distribuci√≥n Normal es Clave?

### Propiedad 1: Suma de Gaussianas = Gaussiana

Si tienes:
- $X \sim \mathcal{N}(\mu_1, \sigma_1^2)$
- $Y \sim \mathcal{N}(\mu_2, \sigma_2^2)$

Entonces:
- $X + Y \sim \mathcal{N}(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)$

**Esto es m√°gico porque**:
```
Imagen (casi normal) + Ruido (normal) = Resultado (tambi√©n normal)
```

Todo el proceso se mantiene en el "mundo gaussiano", lo que lo hace:
- **Matem√°ticamente tratable**
- **F√°cil de modelar**
- **Predecible**

### Propiedad 2: Reversibilidad

Como el proceso es gaussiano:
```
Si sabemos: x_t = ‚àö(1-Œ≤‚Çú) ¬∑ x_{t-1} + ruido_gaussiano
Podemos aprender a invertirlo: x_{t-1} ‚âà f(x_t)
```

Si el ruido fuera ca√≥tico o de otra distribuci√≥n, ser√≠a **mucho m√°s dif√≠cil** aprender a revertirlo.

### Propiedad 3: Convergencia Garantizada

Matem√°ticamente se puede probar que despu√©s de suficientes pasos:

$$x_T \sim \mathcal{N}(0, I) \text{ (ruido puro gaussiano)}$$

No importa qu√© imagen inicial tengas (gato, perro, monta√±a), todas convergen a la misma distribuci√≥n de ruido.

---

## üìä Ejemplo Num√©rico Paso a Paso

Imaginemos un solo pixel de una imagen:

**Paso 0 ‚Üí Paso 1** (con $\beta_1 = 0.01$)

1. **Tenemos**: $x_0 = 0.8$ (pixel gris claro)

2. **Calculamos la media**:
   $$\mu = \sqrt{1-0.01} \times 0.8 = \sqrt{0.99} \times 0.8 = 0.995 \times 0.8 = 0.796$$

3. **Generamos ruido**: $\epsilon \sim \mathcal{N}(0,1)$ ‚Üí supongamos que sale $\epsilon = 0.5$

4. **Calculamos $x_1$**:
   $$x_1 = 0.796 + \sqrt{0.01} \times 0.5 = 0.796 + 0.1 \times 0.5 = 0.796 + 0.05 = 0.846$$

5. **Resultado**: El pixel cambi√≥ de 0.8 a 0.846 (un poquito m√°s claro por el ruido)

---

## üé® Analog√≠a Intuitiva

Imagina que est√°s desenfocando una foto:

- **$x_{t-1}$**: Tu foto actual
- **$\sqrt{1-\beta_t}$**: Un filtro que la reduce ligeramente (√ó0.99)
- **$\sqrt{\beta_t} \epsilon$**: A√±ades unos granitos de arena aleatorios
- **$x_t$**: La foto resultante (ligeramente m√°s borrosa)

Repites esto 1000 veces ‚Üí al final tienes solo arena (ruido puro).

---

## üß† La Distribuci√≥n Normal Aporta 3 Ventajas Cr√≠ticas

### 1. **Reversibilidad** (clave para el modelo)
- El modelo puede aprender a invertir el proceso
- Patrones gaussianos son predecibles

### 2. **Convergencia Garantizada**
- Todas las im√°genes convergen a $\mathcal{N}(0, I)$
- No importa la imagen inicial

### 3. **Facilidad de Muestreo**
- Generar $\epsilon \sim \mathcal{N}(0,1)$ es computacionalmente barato
- Implementado eficientemente en todas las librer√≠as

---

## üéì Resumen Conceptual

### La f√≥rmula garantiza que:

1. **"Controlado"** significa:
   - Varianza constante (no explota)
   - Corrupci√≥n gradual y predecible
   - Convergencia a ruido puro $\mathcal{N}(0,1)$

2. **"Distribuci√≥n Normal"** aporta:
   - Propiedades matem√°ticas √∫tiles
   - Reversibilidad del proceso
   - Facilidad de implementaci√≥n
   - Estabilidad en el entrenamiento

3. **"Evitar corrupci√≥n ca√≥tica"** se traduce en:
   - El modelo puede aprender patrones consistentes
   - No hay valores infinitos o NaNs
   - El proceso es reproducible

---

## üí° Pensamiento Clave

Piensa en la distribuci√≥n normal como el **"idioma universal"** de los modelos de difusi√≥n:

- **Forward process**: Traducir la imagen gradualmente a este idioma (ruido gaussiano)
- **Reverse process**: Traducir de vuelta desde el idioma (ruido) a imagen

Si us√°ramos otro "idioma" (otra distribuci√≥n), todo ser√≠a m√°s complicado y menos estable.

---

## üèóÔ∏è Arquitectura U-Net: El Cerebro del Reverse Process

### ¬øQu√© es U-Net?

**U-Net** es una arquitectura de red neuronal convolucional con forma de "U" que se usa para predecir el ruido en cada paso del reverse process. Fue originalmente dise√±ada para segmentaci√≥n de im√°genes m√©dicas (2015), pero se ha convertido en el est√°ndar para modelos de difusi√≥n.

### Estructura Visual de U-Net

```mermaid
graph TD
    Input["Input: Imagen Ruidosa<br/>28√ó28√ó1 + timestep t"]

    %% Encoder
    C1["Conv + ReLU<br/>28√ó28√ó32"]
    P1["MaxPool 2√ó2<br/>14√ó14√ó32"]
    C2["Conv + ReLU<br/>14√ó14√ó64"]
    P2["MaxPool 2√ó2<br/>7√ó7√ó64"]
    C3["Conv + ReLU<br/>7√ó7√ó128"]
    P3["MaxPool 2√ó2<br/>3√ó3√ó128"]

    %% Bottleneck
    BN["Bottleneck<br/>3√ó3√ó128<br/>+ Time Embedding"]

    %% Decoder
    U1["Upsample<br/>7√ó7√ó128"]
    CC1["Concat<br/>7√ó7√ó256"]
    D1["Conv + ReLU<br/>7√ó7√ó64"]
    U2["Upsample<br/>14√ó14√ó64"]
    CC2["Concat<br/>14√ó14√ó128"]
    D2["Conv + ReLU<br/>14√ó14√ó32"]
    U3["Upsample<br/>28√ó28√ó32"]
    CC3["Concat<br/>28√ó28√ó64"]
    D3["Conv + ReLU<br/>28√ó28√ó32"]

    Output["Output: Ruido Predicho<br/>28√ó28√ó1"]

    %% Flujo principal
    Input --> C1
    C1 --> P1
    P1 --> C2
    C2 --> P2
    P2 --> C3
    C3 --> P3
    P3 --> BN
    BN --> U1
    U1 --> CC1
    CC1 --> D1
    D1 --> U2
    U2 --> CC2
    CC2 --> D2
    D2 --> U3
    U3 --> CC3
    CC3 --> D3
    D3 --> Output

    %% Skip connections
    C3 -.->|Skip Connection| CC1
    C2 -.->|Skip Connection| CC2
    C1 -.->|Skip Connection| CC3

    style Input fill:#e1f5ff
    style Output fill:#ffe1e1
    style BN fill:#fff4e1
    style C1 fill:#e8f5e9
    style C2 fill:#e8f5e9
    style C3 fill:#e8f5e9
    style D1 fill:#f3e5f5
    style D2 fill:#f3e5f5
    style D3 fill:#f3e5f5
```

### Componentes Clave

#### 1. **Encoder (Camino Descendente)** üîΩ
- **Prop√≥sito**: Extraer caracter√≠sticas y comprimir la imagen
- **Operaciones**: Convoluciones + Max Pooling
- **Efecto**: La imagen se hace m√°s peque√±a pero con m√°s canales
- **Ejemplo**: 28√ó28√ó1 ‚Üí 14√ó14√ó32 ‚Üí 7√ó7√ó64 ‚Üí 3√ó3√ó128

#### 2. **Bottleneck (Cuello de Botella)** üéØ
- **Prop√≥sito**: Representaci√≥n m√°s compacta de la informaci√≥n
- **Caracter√≠sticas**: Mayor n√∫mero de canales, menor resoluci√≥n espacial
- **Aqu√≠ se integra**: Time embedding (informaci√≥n sobre el timestep $t$)

#### 3. **Decoder (Camino Ascendente)** üîº
- **Prop√≥sito**: Reconstruir la imagen a resoluci√≥n original
- **Operaciones**: Convoluciones Transpuestas (Upsampling)
- **Efecto**: La imagen vuelve a crecer
- **Ejemplo**: 3√ó3√ó128 ‚Üí 7√ó7√ó64 ‚Üí 14√ó14√ó32 ‚Üí 28√ó28√ó1

#### 4. **Skip Connections (Conexiones de Salto)** üîó
- **El secreto de U-Net**: Conectan encoder directamente con decoder
- **Prop√≥sito**: Preservar detalles finos de la imagen original
- **C√≥mo funcionan**: Concatenan features del encoder con el decoder

### Flujo de Informaci√≥n con Skip Connections

```mermaid
flowchart LR
    subgraph Encoder
        E1[Conv 28√ó28] --> E2[Conv 14√ó14] --> E3[Conv 7√ó7]
    end

    subgraph Bottleneck
        B[3√ó3√ó128<br/>+ Time]
    end

    subgraph Decoder
        D1[Conv 7√ó7] --> D2[Conv 14√ó14] --> D3[Conv 28√ó28]
    end

    E3 --> B
    B --> D1

    E3 -.->|Skip| D1
    E2 -.->|Skip| D2
    E1 -.->|Skip| D3

    style E1 fill:#e8f5e9
    style E2 fill:#e8f5e9
    style E3 fill:#e8f5e9
    style B fill:#fff4e1
    style D1 fill:#f3e5f5
    style D2 fill:#f3e5f5
    style D3 fill:#f3e5f5
```

**¬øPor qu√© son importantes las skip connections?**

```mermaid
graph TB
    subgraph "Sin Skip Connections ‚ùå"
        A1[Input] --> A2[Encoder] --> A3[Bottleneck] --> A4[Decoder] --> A5[Output]
        A6[Problema: Se pierden<br/>detalles finos]
    end

    subgraph "Con Skip Connections ‚úÖ"
        B1[Input] --> B2[Encoder]
        B2 --> B3[Bottleneck]
        B3 --> B4[Decoder]
        B2 -.->|Detalles preservados| B4
        B4 --> B5[Output de alta calidad]
    end

    style A6 fill:#ffe1e1
    style B5 fill:#e1ffe1
```

### Time Embedding en U-Net

```mermaid
graph LR
    T[Timestep t] --> SE[Sinusoidal<br/>Embedding]
    SE --> TE[Time Embedding<br/>128-dim vector]

    X[Imagen Ruidosa] --> Conv[Conv Layer]
    TE --> Linear[Linear Layer]
    Linear --> Add[+]
    Conv --> Add
    Add --> Out[Features con<br/>informaci√≥n temporal]

    style T fill:#e1f5ff
    style TE fill:#fff4e1
    style Out fill:#e1ffe1
```

**¬øPor qu√©?** El modelo necesita saber en qu√© paso $t$ estamos:
- En $t=10$: Hay poco ruido, predicci√≥n debe ser sutil
- En $t=900$: Hay mucho ruido, predicci√≥n debe ser m√°s agresiva

### ¬øPor qu√© U-Net para Modelos de Difusi√≥n?

```mermaid
mindmap
  root((U-Net en<br/>Diffusion))
    Preserva Detalles
      Skip connections
      Informaci√≥n de alta frecuencia
      Bordes y texturas
    Multi-escala
      Patrones globales
      Patrones locales
      M√∫ltiples resoluciones
    Simetr√≠a I/O
      Input 28√ó28
      Output 28√ó28
      Mismo tama√±o
    Eficiente
      Menor costo que Transformers
      Bueno para im√°genes
      Paralelizable
```

### Comparaci√≥n con Otras Arquitecturas

| Arquitectura | Skip Connections | Tama√±o I/O | U-Net en Diffusion |
|--------------|------------------|------------|---------------------|
| **CNN Simple** | ‚ùå No | Diferente | ‚ùå Pierde detalles |
| **ResNet** | ‚úÖ S√≠ (internos) | Diferente | ‚ö†Ô∏è No dise√±ado para esto |
| **U-Net** | ‚úÖ S√≠ (encoder‚Üídecoder) | Mismo | ‚úÖ Perfecto |
| **Transformer** | ‚ùå No (usa atenci√≥n) | Flexible | ‚ö†Ô∏è Costoso para im√°genes |

### Proceso Completo en Diffusion

```mermaid
sequenceDiagram
    participant I as Imagen Ruidosa (x_t)
    participant T as Time Embedding (t)
    participant U as U-Net
    participant N as Ruido Predicho (Œµ)
    participant C as C√°lculo x_(t-1)

    I->>U: Input: x_t
    T->>U: Input: timestep t
    Note over U: Encoder ‚Üí Bottleneck ‚Üí Decoder<br/>con skip connections
    U->>N: Output: Œµ_Œ∏(x_t, t)
    N->>C: Usar para calcular x_(t-1)
    C-->>I: x_(t-1) = (x_t - ruido) / ‚àöŒ±_t
```

### Resumen Visual: La Forma de "U"

```mermaid
graph TD
    subgraph "Alta Resoluci√≥n<br/>Pocos Canales"
        AR1[28√ó28√ó32]
    end

    subgraph "Resoluci√≥n Media<br/>Canales Medios"
        RM1[14√ó14√ó64]
    end

    subgraph "Baja Resoluci√≥n<br/>Muchos Canales"
        BR1[7√ó7√ó128]
        BN[Bottleneck<br/>3√ó3√ó128]
        BR2[7√ó7√ó128]
    end

    subgraph "Resoluci√≥n Media<br/>Canales Medios"
        RM2[14√ó14√ó64]
    end

    subgraph "Alta Resoluci√≥n<br/>Pocos Canales"
        AR2[28√ó28√ó32]
    end

    AR1 --> RM1
    RM1 --> BR1
    BR1 --> BN
    BN --> BR2
    BR2 --> RM2
    RM2 --> AR2

    AR1 -.->|Skip| AR2
    RM1 -.->|Skip| RM2
    BR1 -.->|Skip| BR2

    style AR1 fill:#e8f5e9
    style AR2 fill:#f3e5f5
    style BN fill:#fff4e1
```

### Resumen: ¬øPor qu√© U-Net es el Est√°ndar?

1. ‚úÖ **Arquitectura sim√©trica**: Input = Output en dimensiones
2. ‚úÖ **Skip connections**: Preservan detalles cruciales para denoising
3. ‚úÖ **Multi-escala**: Captura patrones globales y locales
4. ‚úÖ **Eficiente**: Menor costo computacional que transformers para im√°genes
5. ‚úÖ **Probada**: Funciona excelentemente en tareas pixel-a-pixel

En modelos de difusi√≥n, U-Net es el "cerebro" que aprende a mirar una imagen ruidosa y decir: **"este es el ruido que tiene, d√©jame predecirlo para poder quitarlo"**.

---

## üé® Entendiendo los Canales en U-Net

### ¬øQu√© son los Canales?

**Analog√≠a Simple:** Los canales son como "lentes m√°gicos" que miran la misma imagen desde diferentes perspectivas.

```mermaid
graph LR
    IMG[Imagen Original<br/>28√ó28]

    L1[Lente 1:<br/>Detecta bordes<br/>verticales]
    L2[Lente 2:<br/>Detecta bordes<br/>horizontales]
    L3[Lente 3:<br/>Detecta esquinas]
    L32[Lente 32:<br/>Detecta c√≠rculos]

    IMG --> L1
    IMG --> L2
    IMG --> L3
    IMG -.-> L32

    L1 --> OUT[32 Canales<br/>28√ó28√ó32]
    L2 --> OUT
    L3 --> OUT
    L32 --> OUT

    style IMG fill:#e1f5ff
    style OUT fill:#ffe1e1
    style L1 fill:#e8f5e9
    style L2 fill:#e8f5e9
    style L3 fill:#e8f5e9
    style L32 fill:#e8f5e9
```

### Canales en Im√°genes Normales vs Redes Neuronales

#### **Im√°genes RGB (3 canales)**

```
Una foto a color tiene:
- Canal Rojo (R)   ‚Üí Intensidades de rojo
- Canal Verde (G)  ‚Üí Intensidades de verde
- Canal Azul (B)   ‚Üí Intensidades de azul

Dimensiones: [Alto, Ancho, 3]
Ejemplo: [512, 512, 3]
```

#### **Redes Neuronales (N canales)**

En redes neuronales, los canales **NO son colores**, son **detectores de caracter√≠sticas**:

```
32 canales podr√≠an detectar:
- Canal 1:  Bordes horizontales
- Canal 2:  Bordes verticales
- Canal 3:  Esquinas de 90¬∞
- Canal 4:  Curvas suaves
- ...
- Canal 32: Alg√∫n patr√≥n espec√≠fico aprendido

Dimensiones: [Alto, Ancho, 32]
Ejemplo: [28, 28, 32]
```

### De 2D a 3D: El Flujo en U-Net

```mermaid
graph TD
    subgraph "INPUT: Alta Resoluci√≥n"
        I["28√ó28√ó1<br/>Una imagen gris<br/>(Alto √ó Ancho √ó Canales)"]
    end

    subgraph "Despu√©s de Conv1"
        C1["28√ó28√ó32<br/>‚úì Mismo tama√±o espacial<br/>‚úó 32 detectores de caracter√≠sticas"]
    end

    subgraph "Despu√©s de MaxPool"
        P1["14√ó14√ó32<br/>‚úó Mitad del tama√±o<br/>‚úì Mismos detectores"]
    end

    subgraph "Despu√©s de Conv2"
        C2["14√ó14√ó64<br/>‚úì Mismo tama√±o espacial<br/>‚úó 64 detectores m√°s complejos"]
    end

    I -->|"Conv: 1‚Üí32 filtros"| C1
    C1 -->|"MaxPool: √∑2 espacial"| P1
    P1 -->|"Conv: 32‚Üí64 filtros"| C2

    style I fill:#e1f5ff
    style C1 fill:#e8f5e9
    style P1 fill:#fff4e1
    style C2 fill:#ffe1e1
```

### Paso a Paso: ¬øQu√© Ocurre con las Dimensiones?

#### **Paso 1: Input ‚Üí Conv1**

```
28√ó28√ó1  ‚Üí  [Conv 32 filtros]  ‚Üí  28√ó28√ó32

¬øQu√© pas√≥?
- Alto: 28 (se mantiene con padding)
- Ancho: 28 (se mantiene con padding)
- Canales: 1 ‚Üí 32 (¬°AUMENT√ì!)

¬øPor qu√©?
Aplicamos 32 filtros diferentes, cada uno aprende a detectar algo distinto.
Es como tomar 32 "fotograf√≠as especializadas" de la misma imagen.
```

#### **Paso 2: Conv1 ‚Üí MaxPool**

```
28√ó28√ó32  ‚Üí  [MaxPool 2√ó2]  ‚Üí  14√ó14√ó32

¬øQu√© pas√≥?
- Alto: 28 ‚Üí 14 (¬°SE REDUCE A LA MITAD!)
- Ancho: 28 ‚Üí 14 (¬°SE REDUCE A LA MITAD!)
- Canales: 32 (se mantiene)

¬øPor qu√©?
MaxPool toma bloques de 2√ó2 pixeles y se queda con el m√°ximo.
Reduce el tama√±o espacial pero mantiene todos los detectores.
```

#### **Paso 3: MaxPool ‚Üí Conv2**

```
14√ó14√ó32  ‚Üí  [Conv 64 filtros]  ‚Üí  14√ó14√ó64

¬øQu√© pas√≥?
- Alto: 14 (se mantiene)
- Ancho: 14 (se mantiene)
- Canales: 32 ‚Üí 64 (¬°AUMENT√ì OTRA VEZ!)

¬øPor qu√©?
Ahora combinamos los 32 detectores anteriores para aprender
64 patrones M√ÅS COMPLEJOS y abstractos.
```

### Intuici√≥n: ¬øPor qu√© Aumentan los Canales?

**Trade-off: Resoluci√≥n Espacial ‚Üî Complejidad de Caracter√≠sticas**

```mermaid
graph LR
    subgraph "ENCODER: Comprime Espacio, Expande Complejidad"
        E1["Alta Resoluci√≥n<br/>28√ó28<br/>Pocos Canales: 32<br/>üîç Detecta patrones SIMPLES<br/>(l√≠neas, bordes)"]
        E2["Media Resoluci√≥n<br/>14√ó14<br/>Canales Medios: 64<br/>üîç Detecta patrones COMPLEJOS<br/>(formas, texturas)"]
        E3["Baja Resoluci√≥n<br/>7√ó7<br/>Muchos Canales: 128<br/>üîç Detecta conceptos ABSTRACTOS<br/>(partes de objetos)"]
    end

    E1 -->|"Espacial ‚Üì<br/>Canales ‚Üë"| E2
    E2 -->|"Espacial ‚Üì<br/>Canales ‚Üë"| E3

    style E1 fill:#e8f5e9
    style E2 fill:#fff4e1
    style E3 fill:#ffe1e1
```

**Analog√≠a:** An√°lisis de una fotograf√≠a de un gato

```
Nivel 1 (28√ó28√ó32):
  "Veo l√≠neas, bordes, colores b√°sicos"
  Patrones: | ‚Äî / \ ‚óã

Nivel 2 (14√ó14√ó64):
  "Veo formas como c√≠rculos, tri√°ngulos, texturas"
  Patrones: ‚ñ≥ ‚óã ‚ñ¢ rayas, puntos

Nivel 3 (7√ó7√ó128):
  "Veo un ojo, una oreja, bigotes - ¬°es un gato!"
  Patrones: conceptos de partes de animales
```

**Principio Clave:** A medida que **reduces el tama√±o espacial**, necesitas **m√°s canales** para capturar informaci√≥n m√°s compleja y abstracta.

### Tabla Completa: Encoder de U-Net

| Operaci√≥n | Entrada | Salida | ¬øQu√© Detecta? |
|-----------|---------|--------|---------------|
| **Conv 32 filtros** | 28√ó28√ó1 | 28√ó28√ó32 | Patrones b√°sicos (bordes, l√≠neas) |
| **MaxPool 2√ó2** | 28√ó28√ó32 | 14√ó14√ó32 | Reduce tama√±o, mantiene patrones |
| **Conv 64 filtros** | 14√ó14√ó32 | 14√ó14√ó64 | Combina patrones ‚Üí formas complejas |
| **MaxPool 2√ó2** | 14√ó14√ó64 | 7√ó7√ó64 | Reduce tama√±o |
| **Conv 128 filtros** | 7√ó7√ó64 | 7√ó7√ó128 | Combina formas ‚Üí conceptos abstractos |
| **MaxPool 2√ó2** | 7√ó7√ó128 | 3√ó3√ó128 | Representaci√≥n m√°s compacta |

### Visualizaci√≥n: La Pir√°mide de Informaci√≥n

```mermaid
graph TD
    subgraph "INFORMACI√ìN EN U-NET"
        L1["Nivel 1: 28√ó28√ó32<br/>üîç DETALLES FINOS<br/>Muchos pixeles, pocas caracter√≠sticas<br/>Detecta: l√≠neas, puntos, bordes"]

        L2["Nivel 2: 14√ó14√ó64<br/>üîç FORMAS B√ÅSICAS<br/>Menos pixeles, m√°s caracter√≠sticas<br/>Detecta: c√≠rculos, esquinas, texturas"]

        L3["Nivel 3: 7√ó7√ó128<br/>üîç PARTES DE OBJETOS<br/>Pocos pixeles, muchas caracter√≠sticas<br/>Detecta: ojos, orejas, ruedas"]

        L4["Nivel 4: 3√ó3√ó128<br/>üîç CONCEPTOS COMPLETOS<br/>Muy pocos pixeles, m√°xima abstracci√≥n<br/>Detecta: gatos, perros, carros"]
    end

    L1 --> L2
    L2 --> L3
    L3 --> L4

    style L1 fill:#e8f5e9
    style L2 fill:#fff4e1
    style L3 fill:#ffe1e1
    style L4 fill:#ffcccc
```

### Ejemplo Num√©rico Completo

Para un d√≠gito MNIST `28√ó28√ó1`:

#### **Encoder (Compresi√≥n):**

```
INPUT:  28√ó28√ó1    (784 pixeles √ó 1 canal = 784 valores)
   ‚Üì Conv 32
LAYER1: 28√ó28√ó32   (784 pixeles √ó 32 canales = 25,088 valores)
   ‚Üì MaxPool
LAYER2: 14√ó14√ó32   (196 pixeles √ó 32 canales = 6,272 valores)
   ‚Üì Conv 64
LAYER3: 14√ó14√ó64   (196 pixeles √ó 64 canales = 12,544 valores)
   ‚Üì MaxPool
LAYER4: 7√ó7√ó64     (49 pixeles √ó 64 canales = 3,136 valores)
   ‚Üì Conv 128
LAYER5: 7√ó7√ó128    (49 pixeles √ó 128 canales = 6,272 valores)
   ‚Üì MaxPool
BOTTLENECK: 3√ó3√ó128 (9 pixeles √ó 128 canales = 1,152 valores)
```

#### **Decoder (Reconstrucci√≥n):**

```mermaid
sequenceDiagram
    participant BN as Bottleneck<br/>3√ó3√ó128
    participant D1 as Decoder 1<br/>7√ó7√ó64
    participant D2 as Decoder 2<br/>14√ó14√ó32
    participant D3 as Decoder 3<br/>28√ó28√ó1

    BN->>D1: Upsample + Concat con skip
    Note over D1: Recupera detalles de nivel 3
    D1->>D2: Upsample + Concat con skip
    Note over D2: Recupera detalles de nivel 2
    D2->>D3: Upsample + Concat con skip
    Note over D3: Recupera detalles de nivel 1
```

### Analog√≠a Final: Libros y Res√∫menes üìö

```
Libro Completo (28√ó28√ó32):
  üìñ 5000 palabras, conceptos b√°sicos
  "El gato est√° en el jard√≠n junto al √°rbol"

Cap√≠tulo Resumido (14√ó14√ó64):
  üìÑ 500 palabras, ideas m√°s densas
  "Gato en jard√≠n natural"

Abstract (7√ó7√ó128):
  üìã 50 palabras, conceptos muy comprimidos
  "Animal dom√©stico - exterior"

T√≠tulo (3√ó3√ó128):
  üìå 3 palabras, m√°xima abstracci√≥n
  "Gato Exterior"
```

**El truco:** Menos espacio (resoluci√≥n) pero informaci√≥n m√°s "densa" (canales).

### Resumen: Canales en U-Net

| Concepto | Explicaci√≥n |
|----------|-------------|
| **¬øQu√© son?** | Detectores de caracter√≠sticas diferentes |
| **¬øPor qu√© aumentan?** | Para capturar patrones m√°s complejos en menos espacio |
| **Trade-off** | Resoluci√≥n espacial ‚Üì = Canales ‚Üë |
| **Encoder** | Comprime espacio, expande complejidad |
| **Decoder** | Expande espacio, reduce complejidad |
| **Skip connections** | Preservan detalles finos durante reconstrucci√≥n |

---

## üöÄ C√≥mo Usar este Repositorio

### Prerrequisitos

```bash
pip install torch torchvision numpy matplotlib tqdm
pip install diffusers transformers accelerate pillow  # Para el notebook 2
```

### Orden Sugerido

1. **Principiantes**: Empieza con `diffusion_models_presentation.ipynb` para una visi√≥n general
2. **Implementaci√≥n**: Contin√∫a con `01_diffusion_fundamentals.ipynb` para entender los detalles
3. **Aplicaciones**: Termina con `02_text_to_image_diffusion.ipynb` para ver casos de uso reales

---

## üìñ Referencias

### Papers Fundamentales

- **[Denoising Diffusion Probabilistic Models (Ho et al., 2020)](https://arxiv.org/abs/2006.11239)** - El paper fundamental
- **[Deep Unsupervised Learning using Nonequilibrium Thermodynamics (Sohl-Dickstein et al., 2015)](https://arxiv.org/abs/1503.03585)** - Paper original
- **[CLIP (Radford et al., 2021)](https://arxiv.org/abs/2103.00020)** - Text encoder usado en DALL-E
- **[DALL-E 2 (Ramesh et al., 2022)](https://arxiv.org/abs/2204.06125)** - Modelo texto-a-imagen de OpenAI
- **[Latent Diffusion Models / Stable Diffusion (Rombach et al., 2022)](https://arxiv.org/abs/2112.10752)** - Difusi√≥n en espacio latente
- **[Imagen (Saharia et al., 2022)](https://arxiv.org/abs/2205.11487)** - Modelo de Google
- **[Classifier-Free Guidance (Ho & Salimans, 2022)](https://arxiv.org/abs/2207.12598)** - T√©cnica para mejorar generaci√≥n

### Recursos Adicionales

- [Understanding Diffusion Models (Luo, 2022)](https://arxiv.org/abs/2208.11970) - Tutorial completo
- [The Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion) - Implementaci√≥n comentada
- [Stable Diffusion GitHub](https://github.com/Stability-AI/stablediffusion) - C√≥digo oficial
- [Hugging Face Diffusers](https://huggingface.co/docs/diffusers/index) - Librer√≠a de difusi√≥n
- [The Illustrated Stable Diffusion](https://jalammar.github.io/illustrated-stable-diffusion/) - Visualizaci√≥n intuitiva

---

## ü§ù Contribuciones

Si encuentras errores o tienes sugerencias para mejorar estos materiales educativos, si√©ntete libre de contribuir.

---

## üìù Licencia

Materiales educativos para uso acad√©mico.

---

**¬°Feliz aprendizaje sobre Modelos de Difusi√≥n!** üéâ

