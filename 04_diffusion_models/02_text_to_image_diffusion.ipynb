{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de Difusión Texto-a-Imagen: El Enfoque de DALL-E y Stable Diffusion\n",
    "\n",
    "## Objetivo de este Notebook\n",
    "\n",
    "Este notebook explica cómo funcionan los modelos de difusión **texto-a-imagen** como DALL-E 2, Stable Diffusion e Imagen. Aprenderás los componentes clave que permiten generar imágenes a partir de descripciones textuales.\n",
    "\n",
    "### ¿Qué aprenderás?\n",
    "\n",
    "1. Arquitectura de modelos texto-a-imagen\n",
    "2. Cómo funcionan los text encoders (CLIP, T5)\n",
    "3. Conditional diffusion: guiar la generación con texto\n",
    "4. Classifier-free guidance para mejorar calidad\n",
    "5. Latent diffusion: generar en espacio comprimido (Stable Diffusion)\n",
    "6. Implementación práctica usando Stable Diffusion\n",
    "\n",
    "### Pre-requisitos\n",
    "\n",
    "- Completar el notebook 01_diffusion_fundamentals.ipynb\n",
    "- Conocimientos básicos de transformers y embeddings\n",
    "- Familiaridad con PyTorch\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción: De Diffusion a Text-to-Image\n",
    "\n",
    "### Evolución de los Modelos\n",
    "\n",
    "1. **Diffusion Models básicos** (2020): Generan imágenes desde ruido\n",
    "   - No hay control sobre qué se genera\n",
    "   - Ej: DDPM de Ho et al.\n",
    "\n",
    "2. **Conditional Diffusion** (2021): Añade control mediante condiciones\n",
    "   - Puede ser guiado por clases, layouts, etc.\n",
    "   - Ej: Guided Diffusion\n",
    "\n",
    "3. **Text-to-Image Diffusion** (2022): Condicionado por texto natural\n",
    "   - Usa text encoders poderosos (CLIP, T5)\n",
    "   - Ej: DALL-E 2, Imagen, Stable Diffusion\n",
    "\n",
    "4. **Latent Diffusion** (2022): Opera en espacio latente comprimido\n",
    "   - Mucho más eficiente\n",
    "   - Ej: Stable Diffusion\n",
    "\n",
    "### Arquitectura General\n",
    "\n",
    "```\n",
    "Texto → [Text Encoder] → Text Embeddings → [U-Net Condicionada] → Imagen\n",
    "                            ↓\n",
    "                    (guía el denoising)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Instalación e Importación\n",
    "\n",
    "Para este notebook, usaremos la librería `diffusers` de Hugging Face, que proporciona implementaciones de modelos de difusión pre-entrenados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting diffusers\n",
      "  Downloading diffusers-0.35.2-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting torch\n",
      "  Using cached torch-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.24.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: matplotlib in /home/edwaraco/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages (3.10.7)\n",
      "Requirement already satisfied: pillow in /home/edwaraco/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages (12.0.0)\n",
      "Collecting importlib_metadata (from diffusers)\n",
      "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting filelock (from diffusers)\n",
      "  Using cached filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting huggingface-hub>=0.34.0 (from diffusers)\n",
      "  Downloading huggingface_hub-1.1.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy in /home/edwaraco/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages (from diffusers) (2.3.4)\n",
      "Collecting regex!=2019.12.17 (from diffusers)\n",
      "  Downloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/edwaraco/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages (from diffusers) (2.32.5)\n",
      "Collecting safetensors>=0.3.1 (from diffusers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting huggingface-hub>=0.34.0 (from diffusers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/edwaraco/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/edwaraco/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/edwaraco/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.34.0->diffusers)\n",
      "  Using cached fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/edwaraco/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages (from huggingface-hub>=0.34.0->diffusers) (4.15.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.34.0->diffusers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: psutil in /home/edwaraco/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages (from accelerate) (7.1.0)\n",
      "Requirement already satisfied: setuptools in /home/edwaraco/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /home/edwaraco/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
      "  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
      "  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
      "  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
      "  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
      "  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch)\n",
      "  Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch)\n",
      "  Using cached nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
      "  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
      "  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.5.0 (from torch)\n",
      "  Using cached triton-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/edwaraco/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/edwaraco/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/edwaraco/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/edwaraco/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=3 in /home/edwaraco/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/edwaraco/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/edwaraco/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting zipp>=3.20 (from importlib_metadata->diffusers)\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/edwaraco/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/edwaraco/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages (from requests->diffusers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/edwaraco/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages (from requests->diffusers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/edwaraco/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages (from requests->diffusers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/edwaraco/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages (from requests->diffusers) (2025.10.5)\n",
      "Downloading diffusers-0.35.2-py3-none-any.whl (4.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m2.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:02\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0ma \u001b[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "Using cached torch-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
      "Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "Using cached nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Using cached triton-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
      "Using cached torchvision-0.24.0-cp312-cp312-manylinux_2_28_x86_64.whl (8.1 MB)\n",
      "Using cached fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Downloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.5/803.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, zipp, triton, sympy, safetensors, regex, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, hf-xet, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, importlib_metadata, huggingface-hub, tokenizers, nvidia-cusolver-cu12, diffusers, transformers, torch, torchvision, accelerate\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33/33\u001b[0m [accelerate]237m━\u001b[0m \u001b[32m32/33\u001b[0m [accelerate]]ver-cu12]2]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.11.0 diffusers-0.35.2 filelock-3.20.0 fsspec-2025.10.0 hf-xet-1.2.0 huggingface-hub-0.36.0 importlib_metadata-8.7.0 mpmath-1.3.0 networkx-3.5 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 regex-2025.11.3 safetensors-0.6.2 sympy-1.14.0 tokenizers-0.22.1 torch-2.9.0 torchvision-0.24.0 transformers-4.57.1 triton-3.5.0 zipp-3.23.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Instalar dependencias\n",
    "!pip install diffusers transformers accelerate torch torchvision matplotlib pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from diffusers import StableDiffusionPipeline, DDPMScheduler, UNet2DConditionModel\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memoria disponible: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Componente Clave 1: Text Encoder (CLIP)\n",
    "\n",
    "### ¿Qué es CLIP?\n",
    "\n",
    "CLIP (Contrastive Language-Image Pre-training) es un modelo que aprende a asociar texto e imágenes. Fue entrenado con 400 millones de pares (imagen, texto) de internet.\n",
    "\n",
    "**Características clave:**\n",
    "- Convierte texto en embeddings de 768 dimensiones (CLIP-ViT-L/14)\n",
    "- Los embeddings capturan el significado semántico\n",
    "- Textos similares tienen embeddings cercanos en el espacio latente\n",
    "\n",
    "### ¿Cómo se usa en Diffusion?\n",
    "\n",
    "```\n",
    "\"Un gato en un sombrero\" → [CLIP Text Encoder] → [emb₁, emb₂, ..., emb_n]\n",
    "                                                          ↓\n",
    "                                              [U-Net condicionada]\n",
    "                                                          ↓\n",
    "                                                  Imagen del gato\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando CLIP text encoder...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7225dbc5665448abaef0601a4542bc7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "548835e9b5784b60a41981b7a786bbd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ae2171accc49f298299a222bdadee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7852c878aaa74d62a137e7ebf1134c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3731e41c2304e61a4121afa88f31f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaec365fade44dcea0bc7f6ba8d0c742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac3da7ff820469fb1daca53fcca13dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cancellation requested; stopping current tasks.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:626\u001b[39m, in \u001b[36mxet_get\u001b[39m\u001b[34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001b[39m\n\u001b[32m    624\u001b[39m     progress.update(progress_bytes)\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mdownload_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxet_download_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccess_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpiration_unix_epoch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_refresher\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_refresher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCargando CLIP text encoder...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m tokenizer = CLIPTokenizer.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mopenai/clip-vit-large-patch14\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m text_encoder = \u001b[43mCLIPTextModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mopenai/clip-vit-large-patch14\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Función para obtener embeddings de texto\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_text_embeddings\u001b[39m(prompts):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages/transformers/modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages/transformers/modeling_utils.py:4900\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4890\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4891\u001b[39m     gguf_file\n\u001b[32m   4892\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4893\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map.values()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[32m   4894\u001b[39m ):\n\u001b[32m   4895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   4896\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4897\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mloaded from GGUF files.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4898\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4900\u001b[39m checkpoint_files, sharded_metadata = \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4901\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4902\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4903\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4904\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4906\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4907\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4911\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4913\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4914\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_auto_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4920\u001b[39m is_sharded = sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4921\u001b[39m is_quantized = hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages/transformers/modeling_utils.py:1037\u001b[39m, in \u001b[36m_get_resolved_checkpoint_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, is_remote_code, transformers_explicit_filename)\u001b[39m\n\u001b[32m   1022\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1023\u001b[39m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[32m   1024\u001b[39m     cached_file_kwargs = {\n\u001b[32m   1025\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcache_dir\u001b[39m\u001b[33m\"\u001b[39m: cache_dir,\n\u001b[32m   1026\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mforce_download\u001b[39m\u001b[33m\"\u001b[39m: force_download,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1035\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m: commit_hash,\n\u001b[32m   1036\u001b[39m     }\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m     resolved_archive_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[32m   1040\u001b[39m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[32m   1041\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename == _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[32m   1042\u001b[39m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages/transformers/utils/hub.py:322\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    265\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    266\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    267\u001b[39m     **kwargs,\n\u001b[32m    268\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    269\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    271\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages/transformers/utils/hub.py:479\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    478\u001b[39m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m         \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    494\u001b[39m         snapshot_download(\n\u001b[32m    495\u001b[39m             path_or_repo_id,\n\u001b[32m    496\u001b[39m             allow_patterns=full_filenames,\n\u001b[32m   (...)\u001b[39m\u001b[32m    505\u001b[39m             local_files_only=local_files_only,\n\u001b[32m    506\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1007\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    987\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    988\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    989\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1004\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   1005\u001b[39m     )\n\u001b[32m   1006\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1007\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1168\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[32m   1167\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[32m-> \u001b[39m\u001b[32m1168\u001b[39m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.incomplete\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1180\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(pointer_path):\n\u001b[32m   1181\u001b[39m         _create_symlink(blob_path, pointer_path, new_blob=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1720\u001b[39m, in \u001b[36m_download_to_tmp_and_move\u001b[39m\u001b[34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[39m\n\u001b[32m   1718\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_xet_available():\n\u001b[32m   1719\u001b[39m     logger.debug(\u001b[33m\"\u001b[39m\u001b[33mXet Storage is enabled for this repo. Downloading file from Xet Storage..\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1720\u001b[39m     \u001b[43mxet_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1721\u001b[39m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1722\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1723\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1724\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1725\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisplayed_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1726\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1727\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1728\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m constants.HF_HUB_DISABLE_XET:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:621\u001b[39m, in \u001b[36mxet_get\u001b[39m\u001b[34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001b[39m\n\u001b[32m    610\u001b[39m     displayed_filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplayed_filename[:\u001b[32m40\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m(…)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    612\u001b[39m progress_cm = _get_progress_bar_context(\n\u001b[32m    613\u001b[39m     desc=displayed_filename,\n\u001b[32m    614\u001b[39m     log_level=logger.getEffectiveLevel(),\n\u001b[32m   (...)\u001b[39m\u001b[32m    618\u001b[39m     _tqdm_bar=_tqdm_bar,\n\u001b[32m    619\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m621\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m progress_cm \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprogress_updater\u001b[39m(progress_bytes: \u001b[38;5;28mfloat\u001b[39m):\n\u001b[32m    624\u001b[39m         progress.update(progress_bytes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyectos/study/eafit/sem_1/ml_aplicado/talleres/python3.12/lib/python3.12/site-packages/tqdm/std.py:1138\u001b[39m, in \u001b[36mtqdm.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_value, traceback)\u001b[39m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_value, traceback):\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cargar CLIP text encoder\n",
    "print(\"Cargando CLIP text encoder...\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "\n",
    "# Función para obtener embeddings de texto\n",
    "def get_text_embeddings(prompts):\n",
    "    \"\"\"\n",
    "    Convierte texto en embeddings usando CLIP\n",
    "    \n",
    "    Args:\n",
    "        prompts: Lista de strings o un solo string\n",
    "    \n",
    "    Returns:\n",
    "        Text embeddings [batch_size, seq_len, hidden_dim]\n",
    "    \"\"\"\n",
    "    if isinstance(prompts, str):\n",
    "        prompts = [prompts]\n",
    "    \n",
    "    # Tokenizar el texto\n",
    "    text_input = tokenizer(\n",
    "        prompts,\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Obtener embeddings\n",
    "    with torch.no_grad():\n",
    "        text_embeddings = text_encoder(text_input.input_ids.to(device))[0]\n",
    "    \n",
    "    return text_embeddings\n",
    "\n",
    "\n",
    "# Ejemplo: Generar embeddings de diferentes prompts\n",
    "prompts = [\n",
    "    \"A photo of a cat\",\n",
    "    \"A photo of a dog\",\n",
    "    \"A picture of a feline\",  # Sinónimo de cat\n",
    "    \"An image of a canine\"     # Sinónimo de dog\n",
    "]\n",
    "\n",
    "embeddings = get_text_embeddings(prompts)\n",
    "print(f\"\\nShape de embeddings: {embeddings.shape}\")\n",
    "print(f\"[batch_size, sequence_length, hidden_dimension]\")\n",
    "\n",
    "# Calcular similitud entre embeddings (pooled)\n",
    "# Tomamos el embedding del token [CLS] o hacemos mean pooling\n",
    "pooled_embeddings = embeddings.mean(dim=1)  # [batch_size, hidden_dim]\n",
    "\n",
    "# Calcular similitud coseno\n",
    "similarity_matrix = F.cosine_similarity(\n",
    "    pooled_embeddings.unsqueeze(1), \n",
    "    pooled_embeddings.unsqueeze(0), \n",
    "    dim=2\n",
    ")\n",
    "\n",
    "# Visualizar matriz de similitud\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(similarity_matrix.cpu().numpy(), cmap='coolwarm', vmin=0.5, vmax=1.0)\n",
    "plt.colorbar(label='Similitud Coseno')\n",
    "plt.xticks(range(len(prompts)), prompts, rotation=45, ha='right')\n",
    "plt.yticks(range(len(prompts)), prompts)\n",
    "plt.title('Similitud entre Embeddings de Texto (CLIP)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObserva cómo 'cat' y 'feline' tienen alta similitud, al igual que 'dog' y 'canine'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Componente Clave 2: Conditional U-Net\n",
    "\n",
    "### Diferencia con U-Net Básica\n",
    "\n",
    "La U-Net condicionada añade **cross-attention** para incorporar información del texto:\n",
    "\n",
    "```\n",
    "U-Net Básica:          U-Net Condicionada:\n",
    "x_t → [Conv] → output  x_t → [Conv] → [Cross-Attention con texto] → output\n",
    "                                ↑\n",
    "                         Text embeddings\n",
    "```\n",
    "\n",
    "### Cross-Attention Mechanism\n",
    "\n",
    "En cada capa de la U-Net:\n",
    "1. **Query (Q)**: Viene de la imagen ruidosa\n",
    "2. **Key (K) y Value (V)**: Vienen del texto embedding\n",
    "3. **Attention**: $\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d}})V$\n",
    "\n",
    "Esto permite que cada pixel \"preste atención\" a las palabras relevantes del prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar arquitectura conceptual de Cross-Attention\n",
    "from matplotlib.patches import Rectangle, FancyBboxPatch, FancyArrowPatch\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.set_xlim(0, 12)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "# Imagen con ruido (izquierda)\n",
    "img_box = FancyBboxPatch((1, 6), 2, 2, boxstyle=\"round,pad=0.1\",\n",
    "                         facecolor='lightblue', edgecolor='blue', linewidth=2)\n",
    "ax.add_patch(img_box)\n",
    "ax.text(2, 7, 'Imagen\\nRuidosa\\n(x_t)', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Text embedding (izquierda abajo)\n",
    "text_box = FancyBboxPatch((1, 3), 2, 2, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='lightgreen', edgecolor='green', linewidth=2)\n",
    "ax.add_patch(text_box)\n",
    "ax.text(2, 4, 'Text\\nEmbedding', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Query (desde imagen)\n",
    "q_box = FancyBboxPatch((4.5, 6.5), 1.5, 1, boxstyle=\"round,pad=0.05\",\n",
    "                       facecolor='lightyellow', edgecolor='orange', linewidth=1.5)\n",
    "ax.add_patch(q_box)\n",
    "ax.text(5.25, 7, 'Query (Q)\\nde imagen', ha='center', va='center', fontsize=9)\n",
    "\n",
    "# Key y Value (desde texto)\n",
    "k_box = FancyBboxPatch((4.5, 4.5), 1.5, 0.7, boxstyle=\"round,pad=0.05\",\n",
    "                       facecolor='lightcoral', edgecolor='red', linewidth=1.5)\n",
    "ax.add_patch(k_box)\n",
    "ax.text(5.25, 4.85, 'Key (K)', ha='center', va='center', fontsize=9)\n",
    "\n",
    "v_box = FancyBboxPatch((4.5, 3.5), 1.5, 0.7, boxstyle=\"round,pad=0.05\",\n",
    "                       facecolor='lightcoral', edgecolor='red', linewidth=1.5)\n",
    "ax.add_patch(v_box)\n",
    "ax.text(5.25, 3.85, 'Value (V)', ha='center', va='center', fontsize=9)\n",
    "\n",
    "# Cross-Attention\n",
    "attn_box = FancyBboxPatch((7, 5), 2.5, 2, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='lavender', edgecolor='purple', linewidth=2)\n",
    "ax.add_patch(attn_box)\n",
    "ax.text(8.25, 6, 'Cross-\\nAttention', ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Output\n",
    "out_box = FancyBboxPatch((10, 6), 1.5, 2, boxstyle=\"round,pad=0.1\",\n",
    "                         facecolor='lightgreen', edgecolor='green', linewidth=2)\n",
    "ax.add_patch(out_box)\n",
    "ax.text(10.75, 7, 'Features\\nCondicionadas', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Arrows\n",
    "arrow1 = FancyArrowPatch((3, 7), (4.5, 7), arrowstyle='->', lw=2, color='blue')\n",
    "arrow2 = FancyArrowPatch((3, 4), (4.5, 4.85), arrowstyle='->', lw=2, color='green')\n",
    "arrow3 = FancyArrowPatch((3, 4), (4.5, 3.85), arrowstyle='->', lw=2, color='green')\n",
    "arrow4 = FancyArrowPatch((6, 7), (7, 6.5), arrowstyle='->', lw=2, color='orange')\n",
    "arrow5 = FancyArrowPatch((6, 4.85), (7, 6), arrowstyle='->', lw=2, color='red')\n",
    "arrow6 = FancyArrowPatch((6, 3.85), (7, 5.5), arrowstyle='->', lw=2, color='red')\n",
    "arrow7 = FancyArrowPatch((9.5, 6), (10, 7), arrowstyle='->', lw=2, color='purple')\n",
    "\n",
    "for arrow in [arrow1, arrow2, arrow3, arrow4, arrow5, arrow6, arrow7]:\n",
    "    ax.add_patch(arrow)\n",
    "\n",
    "# Formula\n",
    "ax.text(6, 2, r'$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d}})V$', \n",
    "        fontsize=12, ha='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.title('Arquitectura de Cross-Attention en U-Net Condicionada', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"La cross-attention permite que cada pixel de la imagen 'mire' las palabras relevantes del texto.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classifier-Free Guidance (CFG)\n",
    "\n",
    "### Problema\n",
    "\n",
    "Los modelos condicionados a veces ignoran parcialmente el texto y generan imágenes genéricas.\n",
    "\n",
    "### Solución: Classifier-Free Guidance\n",
    "\n",
    "Durante el sampling, calculamos dos predicciones:\n",
    "1. **Conditional**: $\\epsilon_\\theta(x_t, c)$ (con el texto)\n",
    "2. **Unconditional**: $\\epsilon_\\theta(x_t, \\emptyset)$ (sin texto, prompt vacío)\n",
    "\n",
    "Luego combinamos:\n",
    "$$\\hat{\\epsilon}_\\theta = \\epsilon_\\theta(x_t, \\emptyset) + w \\cdot (\\epsilon_\\theta(x_t, c) - \\epsilon_\\theta(x_t, \\emptyset))$$\n",
    "\n",
    "Donde:\n",
    "- $w$ es el **guidance scale** (típicamente 7.5)\n",
    "- $w > 1$: Sigue más el texto (más fidelidad, menos diversidad)\n",
    "- $w = 1$: Sin guidance\n",
    "\n",
    "### Intuición\n",
    "\n",
    "```\n",
    "Unconditional: \"imagen cualquiera\"\n",
    "Conditional:   \"imagen de un gato específico\"\n",
    "                        ↓\n",
    "            Amplificamos la diferencia\n",
    "                        ↓\n",
    "           \"SUPER gato específico\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulación conceptual de CFG\n",
    "import numpy as np\n",
    "\n",
    "# Simular predicciones de ruido (1D para simplicidad)\n",
    "unconditional_noise = np.random.randn(100) * 0.5  # Predicción genérica\n",
    "conditional_noise = np.random.randn(100) * 0.5 + np.sin(np.linspace(0, 4*np.pi, 100))  # Con patrón\n",
    "\n",
    "# Aplicar CFG con diferentes guidance scales\n",
    "guidance_scales = [1.0, 3.0, 7.5, 15.0]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, w in enumerate(guidance_scales):\n",
    "    # Fórmula CFG\n",
    "    guided_noise = unconditional_noise + w * (conditional_noise - unconditional_noise)\n",
    "    \n",
    "    axes[idx].plot(unconditional_noise, label='Unconditional', alpha=0.6, linestyle='--')\n",
    "    axes[idx].plot(conditional_noise, label='Conditional', alpha=0.6, linestyle='--')\n",
    "    axes[idx].plot(guided_noise, label=f'Guided (w={w})', linewidth=2)\n",
    "    axes[idx].set_title(f'Guidance Scale = {w}')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].set_ylim(-5, 5)\n",
    "\n",
    "plt.suptitle('Efecto del Classifier-Free Guidance', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Nota: Con guidance scale alto (w=15), el patrón se amplifica mucho.\")\n",
    "print(\"En práctica, w=7.5 es un buen balance entre fidelidad y calidad.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Latent Diffusion Models (Stable Diffusion)\n",
    "\n",
    "### Problema con Pixel-Space Diffusion\n",
    "\n",
    "Generar imágenes de alta resolución directamente es:\n",
    "- **Muy lento**: 512x512x3 = 786,432 dimensiones\n",
    "- **Costoso**: Requiere GPU de alta gama\n",
    "\n",
    "### Solución: Latent Diffusion\n",
    "\n",
    "Operar en un espacio comprimido:\n",
    "\n",
    "```\n",
    "Imagen (512×512×3) → [VAE Encoder] → Latent (64×64×4) → [Diffusion] → \n",
    "                                                                          ↓\n",
    "Imagen generada ← [VAE Decoder] ← Latent denoised ←─────────────────────┘\n",
    "```\n",
    "\n",
    "**Ventajas:**\n",
    "- 8× reducción en cada dimensión → 64× reducción en memoria\n",
    "- Mucho más rápido\n",
    "- Misma calidad visual\n",
    "\n",
    "### Componentes de Stable Diffusion\n",
    "\n",
    "1. **VAE (Variational Autoencoder)**: Comprime/descomprime imágenes\n",
    "2. **CLIP Text Encoder**: Procesa el texto\n",
    "3. **U-Net Condicionada**: Denoising en espacio latente\n",
    "4. **Scheduler**: Controla el proceso de diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Usar Stable Diffusion Pre-entrenado\n",
    "\n",
    "Ahora usemos un modelo pre-entrenado para generar imágenes reales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando Stable Diffusion...\n",
      "Esto puede tardar varios minutos la primera vez (descarga ~4GB)\n"
     ]
    }
   ],
   "source": [
    "# Cargar Stable Diffusion (versión más liviana)\n",
    "print(\"Cargando Stable Diffusion...\")\n",
    "print(\"Esto puede tardar varios minutos la primera vez (descarga ~4GB)\")\n",
    "\n",
    "# Usar la versión 1.5 o 2.1\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"  # Cambiar a \"stabilityai/stable-diffusion-2-1\" si lo deseas\n",
    "\n",
    "# Cargar pipeline\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16 if device.type == 'cuda' else torch.float32,\n",
    "    safety_checker=None  # Desactivar para experimentación\n",
    ")\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "# Si tienes GPU, habilitar optimizaciones\n",
    "if device.type == 'cuda':\n",
    "    pipe.enable_attention_slicing()  # Reduce uso de memoria\n",
    "    # pipe.enable_xformers_memory_efficient_attention()  # Requiere xformers instalado\n",
    "\n",
    "print(\"Modelo cargado exitosamente!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para generar imágenes\n",
    "def generate_image(prompt, negative_prompt=\"\", guidance_scale=7.5, num_inference_steps=50, seed=None):\n",
    "    \"\"\"\n",
    "    Genera una imagen desde un prompt de texto\n",
    "    \n",
    "    Args:\n",
    "        prompt: Descripción de la imagen a generar\n",
    "        negative_prompt: Qué NO debe aparecer en la imagen\n",
    "        guidance_scale: Qué tan fuerte seguir el prompt (1-20, típicamente 7.5)\n",
    "        num_inference_steps: Número de pasos de denoising (más = mejor calidad pero más lento)\n",
    "        seed: Semilla para reproducibilidad\n",
    "    \n",
    "    Returns:\n",
    "        Imagen generada (PIL Image)\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    else:\n",
    "        generator = None\n",
    "    \n",
    "    with torch.autocast(device.type if device.type == 'cuda' else 'cpu'):\n",
    "        image = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            guidance_scale=guidance_scale,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            generator=generator\n",
    "        ).images[0]\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "# Ejemplos de prompts\n",
    "prompts = [\n",
    "    \"A photo of a cute cat wearing a wizard hat, studio lighting, detailed, 4k\",\n",
    "    \"A beautiful landscape with mountains and a lake at sunset, oil painting style\",\n",
    "    \"A futuristic city with flying cars, cyberpunk, neon lights, highly detailed\",\n",
    "    \"A portrait of a robot reading a book in a library, digital art\"\n",
    "]\n",
    "\n",
    "# Generar imágenes\n",
    "print(\"Generando imágenes...\\n\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, prompt in enumerate(prompts):\n",
    "    print(f\"[{idx+1}/4] Generando: {prompt[:60]}...\")\n",
    "    \n",
    "    image = generate_image(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=\"blurry, bad quality, ugly, distorted\",\n",
    "        guidance_scale=7.5,\n",
    "        num_inference_steps=30,  # Reducido para velocidad, usar 50 para mejor calidad\n",
    "        seed=42 + idx  # Para reproducibilidad\n",
    "    )\n",
    "    \n",
    "    axes[idx].imshow(image)\n",
    "    axes[idx].set_title(prompt[:50] + '...', fontsize=9)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Imágenes Generadas con Stable Diffusion', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGeneración completa!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Experimentos: Efecto de los Parámetros\n",
    "\n",
    "### 8.1 Guidance Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar diferentes guidance scales\n",
    "prompt = \"A photograph of an astronaut riding a horse on mars, highly detailed\"\n",
    "guidance_scales = [1.0, 5.0, 7.5, 15.0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for idx, scale in enumerate(guidance_scales):\n",
    "    print(f\"Generando con guidance_scale={scale}...\")\n",
    "    \n",
    "    image = generate_image(\n",
    "        prompt=prompt,\n",
    "        guidance_scale=scale,\n",
    "        num_inference_steps=30,\n",
    "        seed=123\n",
    "    )\n",
    "    \n",
    "    axes[idx].imshow(image)\n",
    "    axes[idx].set_title(f'Guidance Scale = {scale}', fontsize=11)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle(f'Prompt: \"{prompt}\"', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservaciones:\")\n",
    "print(\"- scale=1.0: Ignora mucho el prompt, imagen más creativa pero menos precisa\")\n",
    "print(\"- scale=7.5: Balance óptimo entre fidelidad y calidad\")\n",
    "print(\"- scale=15.0: Sigue el prompt muy de cerca, puede ser sobre-saturado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Number of Inference Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar diferentes números de steps\n",
    "prompt = \"A serene zen garden with cherry blossoms, Japanese style, peaceful\"\n",
    "steps_list = [10, 20, 30, 50]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for idx, steps in enumerate(steps_list):\n",
    "    print(f\"Generando con {steps} steps...\")\n",
    "    \n",
    "    image = generate_image(\n",
    "        prompt=prompt,\n",
    "        guidance_scale=7.5,\n",
    "        num_inference_steps=steps,\n",
    "        seed=456\n",
    "    )\n",
    "    \n",
    "    axes[idx].imshow(image)\n",
    "    axes[idx].set_title(f'{steps} Steps', fontsize=11)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle(f'Prompt: \"{prompt}\"', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservaciones:\")\n",
    "print(\"- 10 steps: Rápido pero puede tener artefactos\")\n",
    "print(\"- 30 steps: Buen balance velocidad/calidad\")\n",
    "print(\"- 50 steps: Máxima calidad, más lento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Negative Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar con y sin negative prompt\n",
    "prompt = \"A portrait of a person\"\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Sin negative prompt\n",
    "print(\"Generando sin negative prompt...\")\n",
    "image1 = generate_image(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=\"\",\n",
    "    seed=789\n",
    ")\n",
    "axes[0].imshow(image1)\n",
    "axes[0].set_title('Sin Negative Prompt', fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Con negative prompt\n",
    "print(\"Generando con negative prompt...\")\n",
    "image2 = generate_image(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=\"blurry, bad anatomy, deformed, ugly, low quality, pixelated\",\n",
    "    seed=789\n",
    ")\n",
    "axes[1].imshow(image2)\n",
    "axes[1].set_title('Con Negative Prompt', fontsize=12)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.suptitle(f'Prompt: \"{prompt}\"', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEl negative prompt ayuda a evitar características no deseadas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Técnicas Avanzadas de Prompting\n",
    "\n",
    "### 9.1 Estructura de un Buen Prompt\n",
    "\n",
    "```\n",
    "[Sujeto] + [Estilo] + [Detalles] + [Iluminación] + [Calidad]\n",
    "```\n",
    "\n",
    "Ejemplos:\n",
    "- ✅ **Bueno**: \"A majestic lion, digital art, detailed fur, golden hour lighting, 4k, trending on artstation\"\n",
    "- ❌ **Malo**: \"lion\"\n",
    "\n",
    "### 9.2 Palabras Clave Útiles\n",
    "\n",
    "**Calidad:**\n",
    "- highly detailed, 4k, 8k, sharp focus, masterpiece\n",
    "- trending on artstation, award winning\n",
    "\n",
    "**Estilo:**\n",
    "- oil painting, watercolor, digital art, photograph, 3D render\n",
    "- cyberpunk, fantasy, realistic, cartoon, anime\n",
    "\n",
    "**Iluminación:**\n",
    "- studio lighting, golden hour, dramatic lighting, soft lighting\n",
    "- volumetric lighting, rim lighting\n",
    "\n",
    "**Artistas (para estilo):**\n",
    "- by Greg Rutkowski, by Artgerm, by Studio Ghibli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar prompts simples vs detallados\n",
    "prompts_comparison = [\n",
    "    (\"A dragon\", \"Simple\"),\n",
    "    (\"A majestic dragon flying over mountains, fantasy art, highly detailed scales, \"\n",
    "     \"dramatic lighting, volumetric clouds, 4k, trending on artstation\", \"Detallado\")\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "for idx, (prompt, label) in enumerate(prompts_comparison):\n",
    "    print(f\"Generando prompt {label}...\")\n",
    "    \n",
    "    image = generate_image(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=\"blurry, bad quality\",\n",
    "        seed=999\n",
    "    )\n",
    "    \n",
    "    axes[idx].imshow(image)\n",
    "    axes[idx].set_title(f'{label}\\n\"{prompt[:60]}...\"', fontsize=10)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Impacto de Prompts Detallados', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparación: DALL-E 2 vs Stable Diffusion vs Imagen\n",
    "\n",
    "### Arquitecturas\n",
    "\n",
    "| Modelo | Text Encoder | Diffusion Space | Características |\n",
    "|--------|--------------|-----------------|------------------|\n",
    "| **DALL-E 2** | CLIP | Pixel + Latent (GLIDE) | Usa prior diffusion para texto→imagen |\n",
    "| **Stable Diffusion** | CLIP | Latent (VAE) | Open source, eficiente |\n",
    "| **Imagen** | T5 | Pixel | Cascading diffusion (64→256→1024) |\n",
    "\n",
    "### DALL-E 2 Architecture\n",
    "\n",
    "```\n",
    "Texto → [CLIP Text] → Text Embedding → [Prior (Diffusion)] → Image Embedding\n",
    "                                                                      ↓\n",
    "                                              [Decoder (Diffusion)] → Imagen\n",
    "```\n",
    "\n",
    "Dos etapas:\n",
    "1. **Prior**: Convierte texto en CLIP image embedding\n",
    "2. **Decoder**: Convierte image embedding en imagen\n",
    "\n",
    "### Imagen Architecture\n",
    "\n",
    "```\n",
    "Texto → [T5] → Embeddings → [Diffusion 64×64] → [Super-res 256] → [Super-res 1024]\n",
    "```\n",
    "\n",
    "Cascading diffusion:\n",
    "1. Base model: 64×64\n",
    "2. Super-resolution 1: 64→256\n",
    "3. Super-resolution 2: 256→1024\n",
    "\n",
    "### ¿Cuál es mejor?\n",
    "\n",
    "- **DALL-E 2**: Mejor comprensión de texto complejo, creative composition\n",
    "- **Stable Diffusion**: Open source, más controlable, eficiente\n",
    "- **Imagen**: Mejor photorealism, texto en imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Implementación Simplificada: Conditional Diffusion\n",
    "\n",
    "Implementemos un modelo de difusión condicionado simple para entender mejor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleConditionalUNet(nn.Module):\n",
    "    \"\"\"U-Net simple con condicionamiento de texto\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels=1, out_channels=1, text_embed_dim=512, time_embed_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(time_embed_dim, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_embed_dim, time_embed_dim)\n",
    "        )\n",
    "        \n",
    "        # Text projection (para cross-attention simplificada)\n",
    "        self.text_proj = nn.Linear(text_embed_dim, 256)\n",
    "        \n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        \n",
    "        # Cross-attention simplificada (solo en el bottleneck)\n",
    "        self.cross_attn = SimpleCrossAttention(256, 256)\n",
    "        \n",
    "        # Decoder\n",
    "        self.upconv1 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.conv4 = nn.Conv2d(256, 128, 3, padding=1)  # 256 por skip connection\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.conv5 = nn.Conv2d(128, 64, 3, padding=1)\n",
    "        \n",
    "        self.out = nn.Conv2d(64, out_channels, 1)\n",
    "    \n",
    "    def forward(self, x, t, text_embed):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Imagen con ruido [B, C, H, W]\n",
    "            t: Time embeddings [B, time_embed_dim]\n",
    "            text_embed: Text embeddings [B, text_embed_dim]\n",
    "        \"\"\"\n",
    "        # Process time\n",
    "        t_emb = self.time_embed(t)  # [B, time_embed_dim]\n",
    "        \n",
    "        # Process text\n",
    "        text_ctx = self.text_proj(text_embed)  # [B, 256]\n",
    "        \n",
    "        # Encoder\n",
    "        x1 = F.silu(self.conv1(x))\n",
    "        x2 = F.silu(self.conv2(F.max_pool2d(x1, 2)))\n",
    "        x3 = F.silu(self.conv3(F.max_pool2d(x2, 2)))\n",
    "        \n",
    "        # Cross-attention con texto\n",
    "        x3 = self.cross_attn(x3, text_ctx.unsqueeze(1))  # Add seq dimension\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.upconv1(x3)\n",
    "        x = torch.cat([x, x2], dim=1)\n",
    "        x = F.silu(self.conv4(x))\n",
    "        \n",
    "        x = self.upconv2(x)\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = F.silu(self.conv5(x))\n",
    "        \n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "class SimpleCrossAttention(nn.Module):\n",
    "    \"\"\"Cross-attention simplificada\"\"\"\n",
    "    \n",
    "    def __init__(self, query_dim, context_dim):\n",
    "        super().__init__()\n",
    "        self.to_q = nn.Linear(query_dim, query_dim)\n",
    "        self.to_k = nn.Linear(context_dim, query_dim)\n",
    "        self.to_v = nn.Linear(context_dim, query_dim)\n",
    "        self.scale = query_dim ** -0.5\n",
    "    \n",
    "    def forward(self, x, context):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Features [B, C, H, W]\n",
    "            context: Text context [B, seq_len, context_dim]\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Reshape x para attention: [B, H*W, C]\n",
    "        x_flat = x.view(B, C, H*W).permute(0, 2, 1)\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        q = self.to_q(x_flat)  # [B, H*W, C]\n",
    "        k = self.to_k(context)  # [B, seq_len, C]\n",
    "        v = self.to_v(context)  # [B, seq_len, C]\n",
    "        \n",
    "        # Attention\n",
    "        attn = torch.matmul(q, k.transpose(-1, -2)) * self.scale  # [B, H*W, seq_len]\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        \n",
    "        # Apply attention\n",
    "        out = torch.matmul(attn, v)  # [B, H*W, C]\n",
    "        \n",
    "        # Reshape back\n",
    "        out = out.permute(0, 2, 1).view(B, C, H, W)\n",
    "        \n",
    "        # Residual connection\n",
    "        return x + out\n",
    "\n",
    "\n",
    "# Crear modelo de ejemplo\n",
    "conditional_model = SimpleConditionalUNet()\n",
    "print(f\"Modelo condicional creado con {sum(p.numel() for p in conditional_model.parameters())} parámetros\")\n",
    "\n",
    "# Test forward pass\n",
    "x_test = torch.randn(2, 1, 28, 28)\n",
    "t_test = torch.randn(2, 128)\n",
    "text_test = torch.randn(2, 512)\n",
    "\n",
    "output = conditional_model(x_test, t_test, text_test)\n",
    "print(f\"Input shape: {x_test.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"Forward pass exitoso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Resumen y Conceptos Clave\n",
    "\n",
    "### ¿Qué aprendimos?\n",
    "\n",
    "1. **Text Encoding**: CLIP convierte texto en embeddings semánticos\n",
    "   - Captura el significado del prompt\n",
    "   - Permite guiar la generación\n",
    "\n",
    "2. **Conditional U-Net**: Cross-attention integra texto en denoising\n",
    "   - Query: de la imagen\n",
    "   - Key/Value: del texto\n",
    "   - Cada pixel \"mira\" las palabras relevantes\n",
    "\n",
    "3. **Classifier-Free Guidance**: Amplifica el efecto del texto\n",
    "   - Compara conditional vs unconditional\n",
    "   - Guidance scale controla fidelidad vs creatividad\n",
    "\n",
    "4. **Latent Diffusion**: Opera en espacio comprimido\n",
    "   - VAE encoder/decoder\n",
    "   - 64× más eficiente que pixel space\n",
    "   - Stable Diffusion usa este enfoque\n",
    "\n",
    "### Comparación de Modelos\n",
    "\n",
    "- **DALL-E 2**: Prior + Decoder, best text understanding\n",
    "- **Stable Diffusion**: Latent diffusion, open source, efficient\n",
    "- **Imagen**: Cascading diffusion, best photorealism\n",
    "\n",
    "### Mejores Prácticas de Prompting\n",
    "\n",
    "1. Ser descriptivo y específico\n",
    "2. Incluir estilo artístico\n",
    "3. Especificar iluminación y calidad\n",
    "4. Usar negative prompts para evitar defectos\n",
    "5. Experimentar con guidance scale (7.5 es un buen inicio)\n",
    "\n",
    "### Limitaciones\n",
    "\n",
    "- Dificultad con texto en imágenes\n",
    "- A veces ignora parte del prompt\n",
    "- Puede generar anatomías incorrectas\n",
    "- Bias del dataset de entrenamiento\n",
    "\n",
    "### Direcciones Futuras\n",
    "\n",
    "- **Video diffusion**: Generación de videos (Imagen Video, Runway)\n",
    "- **3D diffusion**: Generar modelos 3D (DreamFusion, Magic3D)\n",
    "- **Fast sampling**: Reducir pasos de inferencia (LCM, SDXL Turbo)\n",
    "- **Better control**: ControlNet, IP-Adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Ejercicios Prácticos\n",
    "\n",
    "### Ejercicio 1: Experimentar con Prompts\n",
    "\n",
    "Genera imágenes con diferentes prompts y compara:\n",
    "- Mismo sujeto, diferentes estilos\n",
    "- Misma escena, diferentes iluminaciones\n",
    "\n",
    "### Ejercicio 2: Optimizar Parámetros\n",
    "\n",
    "Para un prompt específico, encuentra:\n",
    "- El mejor guidance_scale\n",
    "- El mínimo número de steps para buena calidad\n",
    "\n",
    "### Ejercicio 3: Negative Prompts\n",
    "\n",
    "Experimenta con diferentes negative prompts:\n",
    "- Genera retratos con/sin negative prompts para anatomía\n",
    "- Compara la calidad\n",
    "\n",
    "### Ejercicio 4: Implementar Training Loop\n",
    "\n",
    "Extiende el `SimpleConditionalUNet` para:\n",
    "- Entrenar en MNIST condicionado por dígitos\n",
    "- Implementar classifier-free guidance\n",
    "\n",
    "### Ejercicio 5: Explorar ControlNet\n",
    "\n",
    "Investiga y prueba ControlNet para:\n",
    "- Generar imágenes condicionadas por edges\n",
    "- Controlar poses humanas\n",
    "\n",
    "## Referencias\n",
    "\n",
    "### Papers Fundamentales\n",
    "\n",
    "- [CLIP (Radford et al., 2021)](https://arxiv.org/abs/2103.00020)\n",
    "- [DALL-E 2 (Ramesh et al., 2022)](https://arxiv.org/abs/2204.06125)\n",
    "- [Imagen (Saharia et al., 2022)](https://arxiv.org/abs/2205.11487)\n",
    "- [Latent Diffusion / Stable Diffusion (Rombach et al., 2022)](https://arxiv.org/abs/2112.10752)\n",
    "- [Classifier-Free Guidance (Ho & Salimans, 2022)](https://arxiv.org/abs/2207.12598)\n",
    "\n",
    "### Recursos Adicionales\n",
    "\n",
    "- [Stable Diffusion GitHub](https://github.com/Stability-AI/stablediffusion)\n",
    "- [Hugging Face Diffusers](https://huggingface.co/docs/diffusers/index)\n",
    "- [The Illustrated Stable Diffusion](https://jalammar.github.io/illustrated-stable-diffusion/)\n",
    "- [ControlNet Paper](https://arxiv.org/abs/2302.05543)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
